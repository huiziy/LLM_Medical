{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import random\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PATH_data_export = \"/Users/huiziyu/Library/CloudStorage/GoogleDrive-huiziy@g.ucla.edu/My Drive/Project - LLM Bib in Biomedical & Health/data/processed\"\n",
    "data = pd.read_csv(f'{PATH_data_export}/relevant_data_gpt.csv')\n",
    "\n",
    "data.head()\n",
    "docs = data[\"Title_abstract\"]\n",
    "# Remove Theme Track, NLP Application \n",
    "zeroshot_topic_list = [\n",
    "    \"Commonsense Reasoning\",\n",
    "    \"Computational Social Science and Cultural Analytics\",\n",
    "    \"Dialogue and Interactive Systems\",\n",
    "    \"Ethics in NLP\",\n",
    "    \"Human-Centered NLP\",\n",
    "    \"Information Extraction\",\n",
    "    \"Information Retrieval and Text Mining\",\n",
    "    \"Interpretability, Interactivity and Analysis of Models for NLP\",\n",
    "    \"Language Modeling and Analysis of Language Models\",\n",
    "    \"Cognitive Modeling and Psycholinguistics\",\n",
    "    \"Multilinguality and Linguistic Diversity\",\n",
    "    \"Natural Language Generation\",\n",
    "    \"Question Answering\",\n",
    "    \"Resources and Evaluation\",\n",
    "    \"Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.\",\n",
    "    \"Sentiment Analysis, Stylistic Analysis, and Argument Mining\",\n",
    "    \"Image, Vision, Video and Multimodality\",\n",
    "    \"Summarization\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P&lt;.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.',\n",
       " 'labels': ['Question Answering',\n",
       "  'Natural Language Generation',\n",
       "  'Resources and Evaluation',\n",
       "  'Information Extraction',\n",
       "  'Language Modeling and Analysis of Language Models',\n",
       "  'Dialogue and Interactive Systems',\n",
       "  'Multilinguality and Linguistic Diversity',\n",
       "  'Information Retrieval and Text Mining',\n",
       "  'Discourse and Pragmatics',\n",
       "  'Commonsense Reasoning',\n",
       "  'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "  'Cognitive Modeling and Psycholinguistics',\n",
       "  'Computational Social Science and Cultural Analytics',\n",
       "  'Semantics: Lexical, Sentence level, Document Level, Textual Inference, etc.',\n",
       "  'Human-Centered NLP',\n",
       "  'Summarization',\n",
       "  'Interpretability, Interactivity and Analysis of Models for NLP',\n",
       "  'Image, Vision, Video and Multimodality',\n",
       "  'Ethics in NLP'],\n",
       " 'scores': [0.13213138282299042,\n",
       "  0.10296814143657684,\n",
       "  0.07934242486953735,\n",
       "  0.07882019877433777,\n",
       "  0.0754040852189064,\n",
       "  0.06728766858577728,\n",
       "  0.05547628179192543,\n",
       "  0.054801128804683685,\n",
       "  0.047910165041685104,\n",
       "  0.03925401717424393,\n",
       "  0.037310149520635605,\n",
       "  0.033315226435661316,\n",
       "  0.03216506168246269,\n",
       "  0.03207553178071976,\n",
       "  0.03151075169444084,\n",
       "  0.028409190475940704,\n",
       "  0.027960624545812607,\n",
       "  0.026681704446673393,\n",
       "  0.01717621088027954]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = docs[1]\n",
    "candidate_labels = zeroshot_topic_list\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/huiziyu/Dropbox/FALL_2023/ResearchCode/LLM_Medical/Analysis/ZeroShot_BERT.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huiziyu/Dropbox/FALL_2023/ResearchCode/LLM_Medical/Analysis/ZeroShot_BERT.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Loop through the batches\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huiziyu/Dropbox/FALL_2023/ResearchCode/LLM_Medical/Analysis/ZeroShot_BERT.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sequences), batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huiziyu/Dropbox/FALL_2023/ResearchCode/LLM_Medical/Analysis/ZeroShot_BERT.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Append the results \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huiziyu/Dropbox/FALL_2023/ResearchCode/LLM_Medical/Analysis/ZeroShot_BERT.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     single_topic_prediction \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m classifier(sequences[i:i\u001b[39m+\u001b[39;49mbatch_size], candidate_labels, hypothesis_template\u001b[39m=\u001b[39;49mhypothesis_template)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to understand extra arguments \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(sequences, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[39mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    267\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(model_forward)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    228\u001b[0m     model_inputs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m    231\u001b[0m model_outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcandidate_label\u001b[39m\u001b[39m\"\u001b[39m: candidate_label,\n\u001b[1;32m    233\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m\"\u001b[39m: sequence,\n\u001b[1;32m    234\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m: inputs[\u001b[39m\"\u001b[39m\u001b[39mis_last\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moutputs,\n\u001b[1;32m    236\u001b[0m }\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1888\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1884\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1885\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1886\u001b[0m     )\n\u001b[0;32m-> 1888\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1889\u001b[0m     input_ids,\n\u001b[1;32m   1890\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1891\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1892\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1893\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1894\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1895\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1896\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1897\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1898\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1899\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1900\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1901\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1902\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1903\u001b[0m )\n\u001b[1;32m   1904\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   1906\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\u001b[39m.\u001b[39mto(hidden_states\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1614\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1607\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1608\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1609\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1610\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1611\u001b[0m     )\n\u001b[1;32m   1613\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1614\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1615\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1616\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1617\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1618\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1619\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1620\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1621\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1622\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1623\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1624\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1625\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1626\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1627\u001b[0m )\n\u001b[1;32m   1629\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1630\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1467\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1454\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1455\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1456\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         use_cache,\n\u001b[1;32m   1465\u001b[0m     )\n\u001b[1;32m   1466\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1467\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1468\u001b[0m         hidden_states,\n\u001b[1;32m   1469\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1470\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1471\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1472\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1473\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   1474\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1475\u001b[0m         ),\n\u001b[1;32m   1476\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1477\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1478\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1479\u001b[0m     )\n\u001b[1;32m   1480\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1482\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:796\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    794\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states))\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_dropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m--> 796\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden_states)\n\u001b[1;32m    797\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    798\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/stats/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tune the batch_size to fit in the memory\n",
    "batch_size = 4 \n",
    "n = 50\n",
    "# Put reviews in a list\n",
    "sequences = random.sample(docs.tolist(), n)\n",
    "\n",
    "# Define the candidate labels \n",
    "candidate_labels = zeroshot_topic_list\n",
    "\n",
    "# Set the hyppothesis template\n",
    "hypothesis_template = \"The topic of this paper is {}.\"\n",
    "\n",
    "# Create an empty list to save the prediciton results\n",
    "single_topic_prediction = []\n",
    "\n",
    "# Loop through the batches\n",
    "for i in range(0, len(sequences), batch_size):\n",
    "    # Append the results \n",
    "    single_topic_prediction += classifier(sequences[i:i+batch_size], candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'predictions' is the list of dictionaries you have as output\n",
    "data = []\n",
    "\n",
    "for prediction in single_topic_prediction:\n",
    "    # Extract the sequence, labels, and scores\n",
    "    sequence = prediction['sequence']\n",
    "    labels = prediction['labels']\n",
    "    scores = prediction['scores']\n",
    "\n",
    "    # Pairing labels with their scores and sorting them\n",
    "    label_score_pairs = sorted(zip(labels, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Taking the top 3 labels and their scores\n",
    "    top_3 = label_score_pairs[:3]\n",
    "\n",
    "    # Creating a row for the DataFrame\n",
    "    row = (\n",
    "        sequence,  # Original text\n",
    "        top_3[0][0], top_3[0][1],  # Topic 1 and its score\n",
    "        top_3[1][0], top_3[1][1],  # Topic 2 and its score\n",
    "        top_3[2][0], top_3[2][1]   # Topic 3 and its score\n",
    "    )\n",
    "    data.append(row)\n",
    "\n",
    "# Creating the DataFrame\n",
    "columns = ['sequence', 'topic_1', 'score_1', 'topic_2', 'score_2', 'topic_3', 'score_3']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# df now contains your formatted data with the original texts\n",
    "\n",
    "## Set Similarity threshold to be 0.1 \n",
    "## each paper could have multiple labels if exceed 0.1\n",
    "## Use example to demonstrate multi label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>score_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>score_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>score_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generative Pre‐Trained Transformer 4 in health...</td>\n",
       "      <td>Ethics in NLP</td>\n",
       "      <td>0.085755</td>\n",
       "      <td>Summarization</td>\n",
       "      <td>0.077525</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.074122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The inevitable transformation of medicine and ...</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>0.081202</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>0.080593</td>\n",
       "      <td>Summarization</td>\n",
       "      <td>0.077747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qilin-Med-VL: Towards Chinese Large Vision-Lan...</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>0.138336</td>\n",
       "      <td>Image, Vision, Video and Multimodality</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>0.094945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The application of Chat Generative Pre-trained...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.123443</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>0.101501</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>0.098464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conversational Chatbot Builder – Smarter Virtu...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>0.117775</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.116473</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>0.109784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Generation of Radiology Findings in Chest X-Ra...</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>0.129765</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.090207</td>\n",
       "      <td>Natural Language Generation</td>\n",
       "      <td>0.087764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Implementing Natural Language Generation throu...</td>\n",
       "      <td>Natural Language Generation</td>\n",
       "      <td>0.912011</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>0.010149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How GPT-3 responds to different publics on cli...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>0.149789</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>0.122365</td>\n",
       "      <td>Commonsense Reasoning</td>\n",
       "      <td>0.120076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Harnessing the Open Access Version of ChatGPT ...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.134291</td>\n",
       "      <td>Language Modeling and Analysis of Language Models</td>\n",
       "      <td>0.092064</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>0.087228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ChatGPT-Enabled daVinci Surgical Robot Prototy...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>0.094912</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>0.094684</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>0.084354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  Generative Pre‐Trained Transformer 4 in health...   \n",
       "1  The inevitable transformation of medicine and ...   \n",
       "2  Qilin-Med-VL: Towards Chinese Large Vision-Lan...   \n",
       "3  The application of Chat Generative Pre-trained...   \n",
       "4  Conversational Chatbot Builder – Smarter Virtu...   \n",
       "5  Generation of Radiology Findings in Chest X-Ra...   \n",
       "6  Implementing Natural Language Generation throu...   \n",
       "7  How GPT-3 responds to different publics on cli...   \n",
       "8  Harnessing the Open Access Version of ChatGPT ...   \n",
       "9  ChatGPT-Enabled daVinci Surgical Robot Prototy...   \n",
       "\n",
       "                            topic_1   score_1  \\\n",
       "0                     Ethics in NLP  0.085755   \n",
       "1                Question Answering  0.081202   \n",
       "2                Question Answering  0.138336   \n",
       "3          Resources and Evaluation  0.123443   \n",
       "4  Dialogue and Interactive Systems  0.117775   \n",
       "5            Information Extraction  0.129765   \n",
       "6       Natural Language Generation  0.912011   \n",
       "7  Dialogue and Interactive Systems  0.149789   \n",
       "8          Resources and Evaluation  0.134291   \n",
       "9  Dialogue and Interactive Systems  0.094912   \n",
       "\n",
       "                                             topic_2   score_2  \\\n",
       "0                                      Summarization  0.077525   \n",
       "1                           Discourse and Pragmatics  0.080593   \n",
       "2             Image, Vision, Video and Multimodality  0.113636   \n",
       "3                             Information Extraction  0.101501   \n",
       "4                           Resources and Evaluation  0.116473   \n",
       "5                           Resources and Evaluation  0.090207   \n",
       "6                           Resources and Evaluation  0.010654   \n",
       "7                           Discourse and Pragmatics  0.122365   \n",
       "8  Language Modeling and Analysis of Language Models  0.092064   \n",
       "9                                 Question Answering  0.094684   \n",
       "\n",
       "                            topic_3   score_3  \n",
       "0          Resources and Evaluation  0.074122  \n",
       "1                     Summarization  0.077747  \n",
       "2            Information Extraction  0.094945  \n",
       "3  Dialogue and Interactive Systems  0.098464  \n",
       "4            Information Extraction  0.109784  \n",
       "5       Natural Language Generation  0.087764  \n",
       "6                Question Answering  0.010149  \n",
       "7             Commonsense Reasoning  0.120076  \n",
       "8          Discourse and Pragmatics  0.087228  \n",
       "9          Resources and Evaluation  0.084354  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT-Enabled daVinci Surgical Robot Prototype: Advancements and Limitations. The daVinci Surgical Robot has revolutionized minimally invasive surgery by enabling greater accuracy and less invasive procedures. However, the system lacks advanced features and autonomy necessary for it to function as a true partner. To enhance its usability, we introduce the implementation of a ChatGPT-based natural language robot interface. Overall, our integration of a ChatGPT-enabled daVinci Surgical Robot has potential to expand the utility of the surgical platform by supplying a more accessible interface. Our system can listen to the operator speak and, through the ChatGPT-enabled interface, translate the sentence and context to execute specific commands to alter the robot’s behavior or to activate certain features. For instance, the surgeon could say (even in Spanish) “please track my left tool” and the system will translate the sentence into a specific track command. This specific error-checked command will then be sent to the hardware which will respond by controlling the camera of the system to continuously adjust to center the left tool in the field of view. We have implemented many commands including “Find my tools” (tools that are not in the field of view) or start/stop recording that can be triggered based on a natural conversational context. Here, we present details of our prototype system, give some accuracy results, and explore its potential implications and limitations. We also discuss how artificial intelligence tools (such as ChatGPT) of the future could be leveraged by robotic surgeons to reduce errors and enhance the efficiency and safety of surgical procedures and even ask for help.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sequence\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Editors' statement on the responsible use of generative artificial intelligence technologies in scholarly journal publishing. The new generative artificial intelligence (AI) tools, and especially the large language models (LLMs) of which ChatGPT is the most prominent example, have the potential to transform many aspects of scholarly publishing. How the transformations will play out remains to be seen, both because the different parties involved in the production and publication of scholarly work are still learning about these tools and because the tools themselves are still in development, but the tools have a vast range of potential uses. Authors are likely to use generative AI to conduct research, frame their thoughts, produce data, search for ways of articulating their thoughts, develop drafts, generate text, revise their writing, and create visuals. Peer reviewers might use AI to help them produce their reviews. Editors might use AI in the initial editorial screening of manuscripts, to locate reviewers, or for copyediting. We are editors of bioethics and humanities journals who have been contemplating the implications of this ongoing transformation. We believe that generative AI may pose a threat to the goals that animate our work but could also be valuable for achieving those goals. We do not pretend to have resolved the many social questions that we think generative AI raises for scholarly publishing, but in the interest of fostering a wider conversation about these questions, we have developed a preliminary set of recommendations about generative AI in scholarly publishing. We hope that the recommendations and rationales set out here will help the scholarly community navigate toward a deeper understanding of the strengths, limits, and challenges of AI for responsible scholarly work. The argument usually given for prohibiting a generative AI tool from being listed as an author is that a requirement of morally responsible publishing is that authors must be accountable for what they write, and generative AI tools lack accountability.1 The publishing industry seems to have reached a consensus that this is a new norm for publishing, which creates a strong presumption in favor of acceptance. While arguments can be made that generative AI possesses some aspects of authorial accountability, such as the capacity to provide an account or explanation of how an article was created, the aspect of accountability that generative AI genuinely lacks is moral responsibility.2 Only persons can be morally responsible, and therefore, if authors must possess moral responsibility, then generative AI cannot be an author. This argument may seem to resolve the question of authorship by sheer stipulation: it accepts in principle that generative AI might (at least eventually) be able to generate content as well as human authors can, and it denies them authorial status by simply asserting that that role is reserved for full members of the moral community. The rationale for this stipulation is that it's required to fulfill our journals’ mission. The goal of our journals is, in part, to foster a community of persons engaged in responsible thinking about ethical and social issues in health care and the biological sciences, not merely to generate publishable papers on those topics. The requirement for accountability is thus grounded both in an understanding of morally responsible publishing and in a goal of creating and protecting a community of people engaged in our work. There are many ways that authors might employ generative AI: to summarize literature, formulate ideas, organize outlines, produce drafts of text, or revise and refine text.3 Some possible uses do not seem significantly different from using internet searches, autocorrect tools, and grammar checks: authors might use generative AI to locate and understand scholarly material and draft text more efficiently. Other uses could influence content and style in novel ways. For example, authors might direct generative AI to propose questions that a paper might address, ideas that a paper might develop, possible outlines for a paper's structure, or alternative phrasing for difficult or ambiguous passages. An author whose primary language is not English might employ generative AI to rewrite a draft and produce a more accessible final version. In these cases, generative AI would serve to produce prompts, suggestions, or foils for the authors' thinking. Yet other uses could raise difficult and maybe novel questions about how ideas and text have been produced and whether they rightly belong to the person. Imagine, for example, that an author used generative AI to mimic the substance and style of another scholar. Questions about plagiarism would arise in such a case, even if no specific passages could be traced to the other scholar. Authors who employ generative AI in developing papers should transparently disclose their use to editors, reviewers, and readers. Since generative AI is constantly changing and the scholarly community is only beginning to experiment with it, it is not prudent at this time to promulgate hard and fast rules for how generative AI should be disclosed. We recommend, however, that disclosure should describe how the AI was used and should identify AI-generated content. Authors should err on the side of too much transparency rather than too little: when in doubt, disclose. Some ways of disclosing the use of generative AI could include describing the use in a paper's introduction, methods section, appendix, or supplemental material or citing the generative AI tool in the notes or references. Although editors must rely on authors to honestly and transparently disclose their use of generative AI, editors should have access (through their publishers or through other services) to tools that can detect whether generative AI was used (and potentially how it was used) in a submitted paper. As with tools that are employed to check for plagiarism, detection tools for generative AI are unlikely to be foolproof. Therefore, the ability of editors to continually draw upon a community of expert reviewers who can raise concerns about an author's use of generative AI will also be essential. Fully transparent disclosure is important for several reasons: To flag potential problems regarding the accuracy of information. For the time being, generative AI is extremely unreliable at offering accurate citations and often makes factual and reasoning errors. In the future, new versions of generative AI and add-ons may be more reliable, but existing systems are, as the name ChatGPT implies, generative transformers of information more than reliable reporters. Authors, readers, and reviewers need to be alerted to, and on guard against, the possible presence of erroneous information. To understand the origin of potential bias within ideas. Generative AI tools may prove to be useful for helping authors collect, organize, and articulate their thoughts. When so used, the technology appears to be analogous to the online platforms and software that gather and analyze data for empirical research reports. Professional scholarly norms dictate that information about these tools be provided to help readers and reviewers evaluate a report. For example, it is common in survey research to cite the use of Qualtrics or REDCap for survey distribution as well as to specify uses of MTurk or specific panels used for survey recruitment. Software such as SPSS, Stata, or Prism are similarly cited when they are used as data analysis tools. Similarly, both for an empirical research report and for a paper that presents conceptual, philosophical analysis, explaining how generative AI has been used to help generate the paper might be necessary to help readers and reviewers evaluate it. To assess ownership and protect the community of scholars. Just as AI image generators can be trained on a visual artist's work and asked to create images in that artist's style, large language models can be trained (so-called fine-tuned) with a writer's work and asked to generate text that mimics that writer, stylistically and substantively. In some cases, such uses will raise questions about plagiarism or intellectual property and about protecting the scholarly community. Creating a dialogue in the style of Plato's Gorgias might be a creative and illuminating exercise for teaching, but generating a paper by training a large language model on the work and style of a living author could harm that author and undermine the community's trust. To support public deliberation about the uptake of generative AI. There are calls from many sources, including from some AI developers, for a broad public conversation about the design and public oversight of these tools, given their implications for the accuracy of shared information and the construction of ideas and considering their potential risks for professional communities. Whatever form such a broad public conversation might take, it depends on a high level of public transparency about the use of generative AI. Any uses editors make of generative AI should also be transparent to authors and should not be the sole basis of reviewer recommendations or editorial decisions. One rationale for this proscription is again to safeguard the editors' role in fostering a community of scholars who are in extended conversation with each other and together sustain and grow their own community of experts. For the time being, given the current state of development of generative AI tools, we do not believe that they are adequate as reviewers.4 However, just as with the creation of content, in the evaluation of a paper, generative AI might be used in a variety of ways; entirely replacing reviewers with AI is only the limit case. An editor might also ask an AI tool whether the concepts or arguments presented in an article have ever appeared in published material. Using AI in this way is similar to running a paper through plagiarism-detection software to determine whether blocks of text have previously been published, even though the use would be intended to gauge conceptual novelty, not to detect actual plagiarism. In light of the potential for improved efficiency and timeliness, there is likely to be pressure by publishers to rely increasingly on AI as a substitute for peer reviewers. Despite the many challenges and difficulties with peer review, we believe that a complete substitution should not take place and urge that publishers retain humans as the final arbiters in the review process. It is also possible that generative AI could be used to identify peer reviewers for manuscripts. Given that many editors already rely on software to suggest peer reviewers and on algorithms to remove conflicts of interest and check for publications in the relevant areas, it would be unsurprising for AI to play a growing role in the editorial process. Again, using AI as a decision-support tool may be beneficial and save time. But replacing this editorial function with AI seems unwarranted, except under exceptional circumstances. There are advantages to having a person—an agent who has moral responsibility for the content of a journal—standing behind all editorial decisions. This has the potential to be lost in new publication models that have moved away from having an editor-in-chief role. It remains to be seen how a sense of editorial responsibility will be distributed in these new publishing models, though ethics audits and greater responsibility by the publisher combined with advisory boards of scholars have been explored. In principle, copyeditors could employ generative AI to improve the language and style of a manuscript and to bring it into conformity with internal guidelines for formatting and references. Such uses do not appear to be substantively different from authors' uses of AI to revise and refine a manuscript in the final stages of preparation, prior to submission. In keeping with the positions above, final responsibility for the text must lie with humans, however. The stance set out here is consistent with those taken by the Committee on Publishing Ethics and many journal publishers, including those that publish or provide publishing services to the journals we edit. However, to our knowledge, previous position statements have not addressed the responsibilities of reviewers to authors.5 Our stance differs from the position of Science magazine, which holds not only that a generative AI tool cannot be an author but also that “text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools.”6 Such a proscription is too broad and may be impossible to enforce, in our view. Yet we recognize that the ethical issues raised by generative AI are complex, and we have struggled to decide how editors should promote responsible use of these technologies. Over time, we hope, the community of scholars will develop professional norms about the appropriate ways of using these new tools. Reviewers and readers, not just editors, will have much to say about these norms. The variety of ways in which generative AI technologies can be used and the pace of change may, in fact, render detailed editorial policy statements ineffective or impracticable. Instead, reliance on evolving professional norms based on broader public conversation about generative AI technologies may turn out to be the best way forward. Our shared statement is intended to promote this wider social discourse. David Resnik's contribution to this editorial was supported by the Intramural Research Program of the National Institute of Environmental Health Sciences (NIEHS) at the National Institutes of Health (NIH). Mohammad Hosseini's contribution was supported by the National Center for Advancing Translational Sciences (NCATS) (through grant UL1TR001422). The funders have not played a role in the design, analysis, decision to publish, or preparation of the manuscript. Veljko Dubljević's contribution was partially supported by the National Science Foundation (NSF) CAREER award (#2043612). This work does not represent the views of the NIEHS, NCATS, NIH, NSF, or US government. Gregory E. Kaebnick is the editor of the Hastings Center Report. David Christopher Magnus is the editor-in-chief of the American Journal of Bioethics. Audiey Kao is the editor-in-chief of the AMA Journal of Ethics. Mohammad Hosseini is associate editor for Accountability in Research. David Resnik is associate editor for Accountability in Research. Veljko Dubljević is the editor-in-chief of the American Journal of Bioethics—Neuroscience. Christy Rentmeester is the managing editor of the AMA Journal of Ethics. Bert Gordijn is a co-editor-in-chief of Medicine, Health Care and Philosophy. Mark J. Cherry is the editor of the Journal of Medicine and Philosophy. Karen J. Maschke is the editor of Ethics & Human Research. Lisa M. Rasmussen is the editor-in-chief of Accountability in Research. Laura Haupt is the managing editor of the Hastings Center Report and Ethics & Human Research. Udo Schüklenk is a co-editor-in-chief of Bioethics and of Developing World Bioethics. Ruth Chadwick is a co-editor-in-chief of Bioethics and the commissioning editor for ethics of the British Medical Bulletin. Debora Diniz is a co-editor-in-chief of Developing World Bioethics.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
